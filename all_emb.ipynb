{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzhigalov/miniconda3/envs/testing/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from summa import summarizer\n",
    "from many_stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATASET.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>cc_article_number</th>\n",
       "      <th>civ_c_article_number</th>\n",
       "      <th>cc_section_number</th>\n",
       "      <th>cc_chapter_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>школа злословие учить прикусывать язык сохраня...</td>\n",
       "      <td>00021 Школа злословия</td>\n",
       "      <td>opencorpora</td>\n",
       "      <td>«Школа злословия» учит прикусить язык Сохранит...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>последний восстание сеул международный биеннал...</td>\n",
       "      <td>00022 Последнее восстание в Сеуле</td>\n",
       "      <td>opencorpora</td>\n",
       "      <td>«Последнее восстание» в Сеуле Международная би...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>отвечать дэвид лисс популярный автор историчес...</td>\n",
       "      <td>00023 За кота - ответишь!</td>\n",
       "      <td>opencorpora</td>\n",
       "      <td>За кота – ответишь! Дэвид Лисс, популярный авт...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>быстротечный кинороман отечественный кинематог...</td>\n",
       "      <td>00024 Быстротечный кинороман</td>\n",
       "      <td>opencorpora</td>\n",
       "      <td>Быстротечный кинороман Отечественные кинематог...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>холодный ванна возвращать сила гидротерапия сч...</td>\n",
       "      <td>00014 Холодная ванна возвращает силы</td>\n",
       "      <td>opencorpora</td>\n",
       "      <td>Холодная ванна возвращает силы Гидротерапия: с...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                             lemmas  \\\n",
       "0    NaN  школа злословие учить прикусывать язык сохраня...   \n",
       "1    NaN  последний восстание сеул международный биеннал...   \n",
       "2    NaN  отвечать дэвид лисс популярный автор историчес...   \n",
       "3    NaN  быстротечный кинороман отечественный кинематог...   \n",
       "4    NaN  холодный ванна возвращать сила гидротерапия сч...   \n",
       "\n",
       "                                   name       source  \\\n",
       "0                 00021 Школа злословия  opencorpora   \n",
       "1     00022 Последнее восстание в Сеуле  opencorpora   \n",
       "2             00023 За кота - ответишь!  opencorpora   \n",
       "3          00024 Быстротечный кинороман  opencorpora   \n",
       "4  00014 Холодная ванна возвращает силы  opencorpora   \n",
       "\n",
       "                                                text  cc_article_number  \\\n",
       "0  «Школа злословия» учит прикусить язык Сохранит...                NaN   \n",
       "1  «Последнее восстание» в Сеуле Международная би...                NaN   \n",
       "2  За кота – ответишь! Дэвид Лисс, популярный авт...                NaN   \n",
       "3  Быстротечный кинороман Отечественные кинематог...                NaN   \n",
       "4  Холодная ванна возвращает силы Гидротерапия: с...                NaN   \n",
       "\n",
       "   civ_c_article_number  cc_section_number  cc_chapter_number  \n",
       "0                   NaN                NaN                NaN  \n",
       "1                   NaN                NaN                NaN  \n",
       "2                   NaN                NaN                NaN  \n",
       "3                   NaN                NaN                NaN  \n",
       "4                   NaN                NaN                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we work only with articles from criminal code and criminal court orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = df[df['source'].isin([\"criminal_code\", \"criminal_court_orders\"])].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tesing purposes\n",
    "#ds = pd.concat([\n",
    "#     ds[ds['source'] == 'criminal_code'].sample(10),\n",
    "#     ds[ds['source'] == 'criminal_court_orders'].sample(10),\n",
    "# ]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Because many texts have many repeating or other useless information we can summarize all texts by `textrank` algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summ_text(text, ratio=0.25, stopwords=get_stop_words(\"ru\"), language=\"russian\"):\n",
    "    return summarizer.summarize(\n",
    "        text, ratio=ratio, \n",
    "        language=language, \n",
    "        additional_stopwords=stopwords\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds['summ'] = ds['text'].map(summ_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.to_csv(\"summ_data.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"summ_data.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty results replaces with text itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds['summ'] = ds.apply(lambda row: row['text'] if pd.isnull(row['summ']) else row['summ'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove \\n, \\r and multiple spaces from texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['summ'] = ds['summ'].str.split().map(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split summarized texts on sentences. If sentence longer than `max_len` parameter, then split it to several chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sent(text, min_len=3, max_len=50):\n",
    "    def split_text(text, max_words=50):\n",
    "        words = text.split()\n",
    "        return [\" \".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "    \n",
    "    res = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        res += list(filter(lambda x: len(x.split()) > min_len, split_text(sent, max_len)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i, row in ds.iterrows():\n",
    "    sents = split_sent(row['summ'])\n",
    "    if sents:\n",
    "        for sent in sents:\n",
    "            res.append({\n",
    "              \"labels\": row[\"labels\"], \n",
    "              \"name\": row[\"name\"],\n",
    "              \"cc_article_number\": row[\"cc_article_number\"],\n",
    "              \"cc_section_number\": row[\"cc_section_number\"],\n",
    "              \"cc_chapter_number\": row[\"cc_chapter_number\"],\n",
    "              \"source\": row[\"source\"],\n",
    "              \"sent\": sent\n",
    "            })\n",
    "\n",
    "s = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ds['cc_article_number'].unique()) == len(s[\"cc_article_number\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple wrapper class for gensim fasttext algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "\n",
    "class FastTextEmbedding:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, sentences: list, model_path=None, corpus_path=None, **kwargs):\n",
    "        \"\"\"Create fasttext model and save to `model_path` (or to tmpfile by default)\"\"\"\n",
    "\n",
    "        def make_corpus():\n",
    "            self.corpus_path = corpus_path\n",
    "            if not self.corpus_path:\n",
    "                _, self.corpus_path = tempfile.mkstemp()\n",
    "            gensim.utils.save_as_line_sentence(sentences, self.corpus_path)\n",
    "            print(\"created corpus at {}\".format(self.corpus_path))\n",
    "            return self.corpus_path\n",
    "\n",
    "        self.model_path = model_path\n",
    "        if not self.model_path:\n",
    "            _, self.model_path = tempfile.mkstemp()\n",
    "\n",
    "        print(\"start training\")\n",
    "        self.model = gensim.models.FastText(corpus_file=make_corpus(), **kwargs)\n",
    "        print(\"training complete\")\n",
    "        print(\"saving model to {}\".format(model_path))\n",
    "        self.model.save(model_path)\n",
    "        print(\"done\")\n",
    "        return self\n",
    "\n",
    "    def sent_emb(self, sentence: str):\n",
    "        \"\"\"Returns mean embedding vector of all word embedding vectors in text\"\"\"\n",
    "\n",
    "        def normalize(arr):\n",
    "            return np.array(arr) / np.linalg.norm(arr)\n",
    "\n",
    "        res = [\n",
    "            normalize(self.model.wv[word])\n",
    "            for word in sentence.split()\n",
    "            if word in self.model.wv\n",
    "        ]\n",
    "        return np.mean(res, axis=0)\n",
    "\n",
    "    def load_model(self, fname):\n",
    "        self.model = gensim.models.FastText.load(fname)\n",
    "        print(\"model loaded\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long part with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "created corpus at fasttext.corpus\n"
     ]
    }
   ],
   "source": [
    "ft_emb = FastTextEmbedding().train(\n",
    "    sentences=s[\"sent\"].str.split().tolist(),\n",
    "    model_path=\"fasttext.model\",\n",
    "    corpus_path=\"fasttext.corpus\",\n",
    "    size=300,\n",
    "    word_ngrams=1,\n",
    "    iter=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastTextEmbedding().load_model(\"fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['fasttext'] = s[\"sent\"].map(ft_emb.sent_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['pca'] = PCA(2).fit_transform(s['fasttext'].tolist()).tolist()\n",
    "\n",
    "\n",
    "METHOD = 'pca'\n",
    "DISPLAY = s['source'].unique()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 7), sharex='all', sharey='all')\n",
    "\n",
    "for source in s['source'].unique():\n",
    "    if source in DISPLAY:\n",
    "        tmp = np.array(s[s['source'] == source][METHOD].tolist())\n",
    "        ax1.scatter(tmp[:, 0], tmp[:, 1], marker='.', label=source, alpha=.4)\n",
    "        ax2 = sns.kdeplot(tmp[:, 0], tmp[:, 1], ax=ax2, legend=False, shade_lowest=False)\n",
    "    \n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "ax2.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.autoscale()\n",
    "\n",
    "plt.savefig(\"ft_pca.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = (\n",
    "    s[s[\"source\"] == \"criminal_code\"]\n",
    "    .groupby(\"name\")\n",
    "    .agg({\n",
    "        \"sent\": list, \n",
    "        \"source\": \"first\", \n",
    "        \"fasttext\": list,\n",
    "        \"cc_article_number\": \"first\",\n",
    "        \"cc_section_number\": \"first\",\n",
    "        \"cc_chapter_number\": \"first\",\n",
    "    }).reset_index(drop=False)\n",
    ")\n",
    "co = (\n",
    "    s[s[\"source\"] == \"criminal_court_orders\"]\n",
    "    .groupby(\"name\")\n",
    "    .agg({\"labels\": \"first\", \"sent\": list, \"source\": \"first\", \"fasttext\": list})\n",
    "    .reset_index(drop=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constellation divergency\n",
    "For measuring distance between texts in latent spaces we introduce new formula:\n",
    "\n",
    "In example we have two texts $a$ and $b$ consists of sentences:  $a_1, a_2, ..., a_N$ и $b_1, b_2, ..., b_M$ embedded in some space.\n",
    "\n",
    "We set some measure of distance between two sentences: $d_{i, j}$.\n",
    "\n",
    "**Сonstellation divergency** of text $a$ to text $b$ is given by:\n",
    "\n",
    "$$\n",
    "\\rho_{a, b} = \\frac{1}{N}\\sum^{N}_{i=1} \\min_{j=1,...,M} d_{i, j}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_distance(text_a, text_b, sent_dist=scipy.spatial.distance.cosine):\n",
    "    \"\"\"Calculate distance between two embedded texts.\n",
    "    params:\n",
    "        text_a - first text\n",
    "        text_b - second text\n",
    "        sent_dist - function that takes two arguments - two vectors and calculate distance between them.\n",
    "    \"\"\"\n",
    "    dist = np.zeros((len(text_a), len(text_b)))\n",
    "    for i, sent_a in enumerate(text_a):\n",
    "        for j, sent_b in enumerate(text_b):\n",
    "            dist[i, j] = sent_dist(sent_a, sent_b)\n",
    "    return dist.min(axis=1).sum() / len(text_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# distances = np.zeros((cc.shape[0], co.shape[0]))\n",
    "\n",
    "# for i, text_a in cc['fasttext'].iteritems():\n",
    "#     for j, text_b in co['fasttext'].iteritems():\n",
    "#         distances[i, j] = text_distance(text_a, text_b, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_sample = co.sample(1000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.zeros((co_sample.shape[0], cc.shape[0]))\n",
    "\n",
    "for i, (_, text_a) in enumerate(co_sample['fasttext'].iteritems()):\n",
    "    if i % 10 == 0:\n",
    "        print( i, \"rows processed\")\n",
    "    for j, text_b in cc['fasttext'].iteritems():\n",
    "        distances[i, j] = text_distance(text_a, text_b, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 1, figsize=(15, 15))\n",
    "sns.heatmap(distances)\n",
    "plt.savefig(\"ft_distances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = np.array(np.unique(np.argpartition(distances, kth=5)[:, :5].flatten(), return_counts=True)).T\n",
    "for x in np.array(sorted(tmp, key=lambda x: x[1], reverse=True))[:, 0][:5]:\n",
    "    print(cc[\"name\"].iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_sample[\"labels\"] = co_sample['labels'].map(lambda x: list(map(float, x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def averaged_accuracy(y_true, y_pred):\n",
    "#     return len(set(y_true) & set(y_pred)) / len(set(y_true) | set(y_pred))\n",
    "\n",
    "# def exact_accuracy(y_true, y_pred):\n",
    "#     return 1 if set(y_true) == set(y_pred) else 0\n",
    "\n",
    "def weak_accuracy(y_true, y_pred):\n",
    "    return 1 if set(y_true) & set(y_pred) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ft = []\n",
    "\n",
    "for N in range(cc.shape[0]):\n",
    "    targets = []\n",
    "    for i in range(co_sample.shape[0]):\n",
    "        targets.append(cc.iloc[np.argpartition(distances, kth=N)[:, :N][i]]['cc_article_number'].tolist())\n",
    "\n",
    "    co_sample['targets'] = targets\n",
    "    weak_acc = co_sample.apply(lambda row: weak_accuracy(row[\"targets\"], row[\"labels\"]), axis=1).mean()\n",
    "    \n",
    "    results_ft.append((N, weak_acc))\n",
    "results_ft = np.array(results_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "plt.plot(results_ft[:, 1], label=\"Weak accuracy\")\n",
    "plt.plot(results_ft[:, 0] / cc.shape[0], label=\"Size of neighbourhood, N\")\n",
    "plt.title(\"Change in accuracy \\nwith changing the number of nearest articles\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size of neighbourhood\")\n",
    "plt.ylabel(\"\")\n",
    "plt.grid()\n",
    "plt.savefig(\"ft_wa.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph structure of criminal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_matrix = np.zeros((cc.shape[0], cc.shape[0]))\n",
    "\n",
    "for i, text_a in cc['fasttext'].iteritems():\n",
    "    for j, text_b in cc['fasttext'].iteritems():\n",
    "        dist_matrix[i, j] = text_distance(text_a, text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dist_matrix.flatten(), bins=30)\n",
    "plt.title(\"Distances between graph nodes\")\n",
    "plt.savefig(\"ft_graph_hist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.from_pandas_adjacency(\n",
    "    pd.DataFrame(\n",
    "        dist_matrix, \n",
    "        index=cc['cc_article_number'], \n",
    "        columns=cc['cc_article_number']\n",
    "    )\n",
    ")\n",
    "\n",
    "chapters = {p[0]:p[1] for p in zip(cc['cc_article_number'].tolist(), cc['cc_chapter_number'].tolist())}\n",
    "sections = {p[0]:p[1] for p in zip(cc['cc_article_number'].tolist(), cc['cc_section_number'].tolist())}\n",
    "\n",
    "nx.set_node_attributes(graph, chapters, name='chapter_number')\n",
    "nx.set_node_attributes(graph, sections, name='section_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(graph, 'fasttext_graph_full.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "def plot_graph(threshold):\n",
    "    def filter_matrix(m, t):\n",
    "        return (m < t) * m\n",
    "    \n",
    "    graph = nx.from_pandas_adjacency(\n",
    "        pd.DataFrame(\n",
    "            filter_matrix(dist_matrix, threshold), \n",
    "            index=cc['cc_article_number'], \n",
    "            columns=cc['cc_article_number']\n",
    "        )\n",
    "    )\n",
    "    deg = graph.degree()\n",
    "    sub = graph.subgraph([n for n in graph.nodes if deg[n] != 0])\n",
    "    colors=list(map(lambda x: cc[cc['cc_article_number'] == x]['cc_section_number'].values[0], sub.nodes))\n",
    "    return nx.draw_networkx(sub, \n",
    "#         pos=nx.drawing.kamada_kawai_layout(sub),\n",
    "                            pos=nx.spring_layout(sub),\n",
    "                            font_color='black', \n",
    "                            font_size=12, \n",
    "                            font_weight='bold', \n",
    "                            edge_color='grey', \n",
    "                            node_size=100,\n",
    "                            node_color=colors,\n",
    "                            cmap=cm.Set1\n",
    "                           )\n",
    "\n",
    "t = [0.03, 0.02, 0.01]\n",
    "fig = plt.figure(figsize=(13, 45))\n",
    "for i, ti in enumerate(t):\n",
    "    fig.add_subplot(4, 1, i+1)\n",
    "    plt.title('threshold={}'.format(ti))\n",
    "    plot_graph(ti)\n",
    "plt.savefig(\"ft_graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pymystem3 import Mystem\n",
    "from many_stop_words import get_stop_words\n",
    "import gensim\n",
    "\n",
    "\n",
    "class doc2vecEmbedding:\n",
    "    def __init__(self, model_path=None, phraser_path=None, corpus_path=None,\n",
    "                 do_phrase=True, stopwords=None):\n",
    "        \n",
    "        self.model_path = model_path\n",
    "        if not self.model_path:\n",
    "            _, self.model_path = tempfile.mkstemp()\n",
    "\n",
    "        self.corpus_path = corpus_path\n",
    "        if not self.corpus_path:\n",
    "            _, self.corpus_path = tempfile.mkstemp()  \n",
    "    \n",
    "        self.do_phrase = do_phrase\n",
    "        if self.do_phrase:\n",
    "            self.phraser_path = phraser_path\n",
    "            if not phraser_path:\n",
    "                _, self.phraser_path = tempfile.mkstemp()\n",
    "        self.stemmer = Mystem(entire_input=False)\n",
    "        self.stopwords = stopwords\n",
    "        if not self.stopwords:\n",
    "            self.stopwords = get_stop_words(\"ru\")\n",
    "    \n",
    "    def lemmatize_and_filter(self, text, min_len=2):\n",
    "        text = self.stemmer.lemmatize(text)\n",
    "        return \" \".join(list(filter(lambda word: word not in self.stopwords and len(word) > min_len, text)))\n",
    "    \n",
    "    def train(self, sentences: list, **kwargs):\n",
    "        \"\"\"Create doc2vec model, save it to `self.model_path` (or to tmpfile by default) and return doc vectors\"\"\"\n",
    "        \n",
    "        def train_phraser(sentences):\n",
    "            phrases = gensim.models.phrases.Phrases(sentences=list(map(str.split, sentences)))\n",
    "            self.phraser = gensim.models.phrases.Phraser(phrases)\n",
    "            self.phraser.save(self.phraser_path)\n",
    "            print(\"Phraser saved to {}\".format(self.phraser_path))\n",
    "        \n",
    "        def make_corpus(sentences):\n",
    "            gensim.utils.save_as_line_sentence(list(map(str.split, sentences)), self.corpus_path)\n",
    "            print(\"Corpus saved to {}\".format(self.corpus_path))\n",
    "        \n",
    "        print(\"Filtering and lemmatizing\")\n",
    "        sentences = list(map(self.lemmatize_and_filter, sentences))\n",
    "        print(\"Done filtering and lemmatizing texts\")\n",
    "        if self.do_phrase:\n",
    "            print(\"Training phraser model\")\n",
    "            train_phraser(sentences)\n",
    "            print(\"Done training phraser model. Phrasing texts\")\n",
    "            sentences = list(map(lambda sent: \" \".join(self.phraser[sent.split()]), sentences))\n",
    "            print(\"Texts phrased\")\n",
    "                                        \n",
    "        make_corpus(sentences)\n",
    "        print(\"Training doc2vec model\")\n",
    "        self.model = gensim.models.doc2vec.Doc2Vec(corpus_file=self.corpus_path, **kwargs)\n",
    "        self.model.save(self.model_path)\n",
    "        print(\"Model saved to {}\".format(self.model_path))\n",
    "        print(\"Training complete\")\n",
    "        return self.model.docvecs.vectors_docs.tolist()\n",
    "\n",
    "    def sent_emb(self, sentence: str):\n",
    "        return self.model.infer_vector(sentence)\n",
    "    \n",
    "    def load_phraser(self):\n",
    "        return gensim.models.phrases.Phraser.load(self.phraser_path)\n",
    "\n",
    "    def load_model(self, model_path, do_phrase=True, phraser_path=None):\n",
    "        self.do_phrase = do_phrase\n",
    "        if self.do_phrase:\n",
    "            assert phraser_path, \"Please set correct phraser_path\"\n",
    "            self.phraser_path = phraser_path\n",
    "            self.phraser = self.load_phraser()\n",
    "            print(\"phraser_loaded\")\n",
    "        self.model_path = model_path\n",
    "        self.model = gensim.models.doc2vec.Doc2Vec.load(self.model_path)\n",
    "        print(\"model loaded\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = doc2vecEmbedding(\n",
    "    model_path=\"doc2vec_model/doc2vec.model\",\n",
    "    phraser_path=\"doc2vec_model/phraser\",\n",
    "    corpus_path=\"doc2vec_model/corpus\"\n",
    ")\n",
    "\n",
    "word_vectors = d2v.train(\n",
    "    sentences=s['sent'],\n",
    "    epochs=100, vector_size=300, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s[\"doc2vec\"] = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s['pca'] = PCA(2).fit_transform(s['doc2vec'].tolist()).tolist()\n",
    "\n",
    "\n",
    "METHOD = 'pca'\n",
    "DISPLAY = s['source'].unique()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 7), sharex='all', sharey='all')\n",
    "\n",
    "for source in s['source'].unique():\n",
    "    if source in DISPLAY:\n",
    "        tmp = np.array(s[s['source'] == source][METHOD].tolist())\n",
    "        ax1.scatter(tmp[:, 0], tmp[:, 1], marker='.', label=source, alpha=.4)\n",
    "        ax2 = sns.kdeplot(tmp[:, 0], tmp[:, 1], ax=ax2, legend=False, shade_lowest=False)\n",
    "    \n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "ax2.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.autoscale()\n",
    "\n",
    "plt.savefig(\"d2v_pca.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = (\n",
    "    s[s[\"source\"] == \"criminal_code\"]\n",
    "    .groupby(\"name\")\n",
    "    .agg({\n",
    "        \"sent\": list, \n",
    "        \"source\": \"first\", \n",
    "        \"doc2vec\": list,\n",
    "        \"cc_article_number\": \"first\",\n",
    "        \"cc_section_number\": \"first\",\n",
    "        \"cc_chapter_number\": \"first\",\n",
    "    }).reset_index(drop=False)\n",
    ")\n",
    "co = (\n",
    "    s[s[\"source\"] == \"criminal_court_orders\"]\n",
    "    .groupby(\"name\")\n",
    "    .agg({\"labels\": \"first\", \"sent\": list, \"source\": \"first\", \"doc2vec\": list})\n",
    "    .reset_index(drop=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_sample = co.sample(1000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.zeros((co_sample.shape[0], cc.shape[0]))\n",
    "\n",
    "for i, (_, text_a) in enumerate(co_sample['doc2vec'].iteritems()):\n",
    "    if i % 10 == 0:\n",
    "        print( i, \"rows processed\")\n",
    "    for j, text_b in cc['doc2vec'].iteritems():\n",
    "        distances[i, j] = text_distance(text_a, text_b, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 1, figsize=(15, 15))\n",
    "sns.heatmap(distances)\n",
    "plt.savefig(\"d2v_distances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array(np.unique(np.argpartition(distances, kth=5)[:, :5].flatten(), return_counts=True)).T\n",
    "for x in np.array(sorted(tmp, key=lambda x: x[1], reverse=True))[:, 0][:5]:\n",
    "    print(cc[\"name\"].iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_sample[\"labels\"] = co_sample['labels'].map(lambda x: list(map(float, x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d2v = []\n",
    "\n",
    "for N in range(cc.shape[0]):\n",
    "    targets = []\n",
    "    for i in range(co_sample.shape[0]):\n",
    "        targets.append(cc.iloc[np.argpartition(distances, kth=N)[:, :N][i]]['cc_article_number'].tolist())\n",
    "\n",
    "    co_sample['targets'] = targets\n",
    "    weak_acc = co_sample.apply(lambda row: weak_accuracy(row[\"targets\"], row[\"labels\"]), axis=1).mean()\n",
    "    \n",
    "    results_d2v.append((N, weak_acc))\n",
    "results_d2v = np.array(results_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "# plt.plot(results[:, 1], label=\"Accuracy\")\n",
    "# plt.plot(results[:, 2], label=\"Exact accuracy\")\n",
    "plt.plot(results_d2v[:, 1], label=\"Weak accuracy\")\n",
    "plt.plot(results_d2v[:, 0] / cc.shape[0], label=\"Size of neighbourhood, N\")\n",
    "plt.title(\"Change in accuracy \\nwith changing the number of nearest articles\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size of neighbourhood\")\n",
    "plt.ylabel(\"\")\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"d2v_wa.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec vs. fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "# plt.plot(results[:, 1], label=\"Accuracy\")\n",
    "# plt.plot(results[:, 2], label=\"Exact accuracy\")\n",
    "plt.plot(results_d2v[:, 1], label=\"doc2vec - Weak accuracy\")\n",
    "plt.plot(results_ft[:, 1], label=\"fasttext - Weak accuracy\")\n",
    "plt.plot(results_d2v[:, 0] / cc.shape[0], label=\"Size of neighbourhood, N\")\n",
    "plt.title(\"Change in accuracy \\nwith changing the number of nearest articles\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size of neighbourhood\")\n",
    "plt.ylabel(\"\")\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"ft_vs_d2v.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (testing)",
   "language": "python",
   "name": "testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
